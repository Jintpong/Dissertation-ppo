diff --git a/.DS_Store b/.DS_Store
index f978a5d7..be44a9ad 100644
Binary files a/.DS_Store and b/.DS_Store differ
diff --git a/eval_output/combined_irrigations.png b/eval_output/combined_irrigations.png
deleted file mode 100644
index 4f9412d4..00000000
Binary files a/eval_output/combined_irrigations.png and /dev/null differ
diff --git a/eval_output/combined_profits.png b/eval_output/combined_profits.png
deleted file mode 100644
index f7c1c7d6..00000000
Binary files a/eval_output/combined_profits.png and /dev/null differ
diff --git a/eval_output/combined_water_efficiency.png b/eval_output/combined_water_efficiency.png
deleted file mode 100644
index cc49fc78..00000000
Binary files a/eval_output/combined_water_efficiency.png and /dev/null differ
diff --git a/eval_output/combined_yields.png b/eval_output/combined_yields.png
deleted file mode 100644
index 8620b319..00000000
Binary files a/eval_output/combined_yields.png and /dev/null differ
diff --git a/eval_output/comparison_results.csv b/eval_output/comparison_results.csv
deleted file mode 100644
index 3440f409..00000000
--- a/eval_output/comparison_results.csv
+++ /dev/null
@@ -1,6 +0,0 @@
-Dry yield (tonne/ha)_mean,Dry yield (tonne/ha)_std,Seasonal irrigation (mm)_mean,Seasonal irrigation (mm)_std,Profit_mean,WaterEfficiency_mean,label
-5.85971667578193,0.0,2350.0,0.0,-1560.8623313279754,2.4934964577795444,Random
-6.225729690965115,0.8128153761848285,424.7610390420007,59.71305791299318,444.8994929703247,14.657016813515964,Interval
-4.901967631939847,1.1948936697723933,0.0,0.0,578.4328790267664,,Rainfed
-6.217823281189286,0.8157080621235794,210.8108108108108,111.10876499024575,657.110311050832,29.494802744103023,Thresholds
-5.859416534539744,0.0,333.3333333333333,0.0,455.73830426541053,17.578249603619234,PPO
diff --git a/eval_output/ppo_timesteps_results.csv b/eval_output/ppo_timesteps_results.csv
deleted file mode 100644
index a5b75b62..00000000
--- a/eval_output/ppo_timesteps_results.csv
+++ /dev/null
@@ -1,4 +0,0 @@
-Dry yield (tonne/ha)_mean,Dry yield (tonne/ha)_std,Seasonal irrigation (mm)_mean,Seasonal irrigation (mm)_std,Profit_mean,WaterEfficiency_mean,label
-5.859744909646445,0.0,341.6666666666667,0.0,447.47721345555124,17.15047290628228,PPO_500000
-5.859416534539744,0.0,333.3333333333333,0.0,455.73830426541053,17.578249603619234,PPO_1000000
-5.858285890255604,0.0,333.3333333333333,0.0,455.48956252289963,17.57485767076681,PPO_2000000
diff --git a/ppo2.py b/ppo2.py
index e4b584ab..7211388d 100644
--- a/ppo2.py
+++ b/ppo2.py
@@ -1,16 +1,14 @@
 import warnings
 from typing import Any, ClassVar, Optional, TypeVar, Union
-
 import numpy as np
 import torch as th
 from gymnasium import spaces
 from torch.nn import functional as F
-from ppo2 import PPO 
 from stable_baselines3.common.buffers import RolloutBuffer
 from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm
 from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy
 from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule
-from stable_baselines3.common.utils import FloatSchedule, explained_variance
+from stable_baselines3.common.utils import explained_variance
 
 SelfPPO = TypeVar("SelfPPO", bound="PPO")
 
@@ -84,9 +82,7 @@ class PPO(OnPolicyAlgorithm):
         # Sanity check, otherwise it will lead to noisy gradient and NaN
         # because of the advantage normalization
         if normalize_advantage:
-            assert (
-                batch_size > 1
-            ), 
+            assert batch_size > 1
 
         if self.env is not None:
             # Check that `n_steps * n_envs > 1` to avoid NaN
@@ -108,8 +104,8 @@ class PPO(OnPolicyAlgorithm):
                 )
         self.batch_size = batch_size
         self.n_epochs = n_epochs
-        self.clip_range = clip_range
-        self.clip_range_vf = clip_range_vf
+        self.clip_range = lambda _: self.clip_range
+        self.clip_range_vf = lambda _: self.clip_range_vf
         self.normalize_advantage = normalize_advantage
         self.target_kl = target_kl
 
@@ -136,7 +132,7 @@ class PPO(OnPolicyAlgorithm):
         # Update optimizer learning rate
         self._update_learning_rate(self.policy.optimizer)
         # Compute current clip range
-        clip_range = self.clip_range(self._current_progress_remaining)  # type: ignore[operator]
+        clip_range = self.clip_range(self._current_progress_remaining)
         # Optional: clip range for the value function
         if self.clip_range_vf is not None:
             clip_range_vf = self.clip_range_vf(self._current_progress_remaining)  # type: ignore[operator]
diff --git a/train.py b/train.py
index 207249fb..727a73bf 100644
--- a/train.py
+++ b/train.py
@@ -3,7 +3,7 @@ import torch
 import numpy as np
 import matplotlib.pyplot as plt
 import wandb
-from stable_baselines3 import PPO
+from ppo2 import PPO
 from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
 from stable_baselines3.common.callbacks import BaseCallback
 from stable_baselines3.common.monitor import Monitor
@@ -19,7 +19,8 @@ wandb.init(
     project="ppo-irrigation",
     name="PPO-Wheat",
     monitor_gym=True,
-    save_code=True
+    save_code=True,
+    reinit=True 
 )
 
 def make_env():
@@ -100,7 +101,7 @@ class RewardLogging(BaseCallback):
             plt.close()
 
 # --- Training loop ---
-timestep_values = [500_000, 1_000_000, 1_500_000, 2_000_000, 2_500_000]
+timestep_values = [500_000, 1_000_000, 2_000_000]
 
 for train_timesteps in timestep_values:
     def make_env():
@@ -123,13 +124,14 @@ for train_timesteps in timestep_values:
         "MlpPolicy",
         train_env,
         device=device,
-        learning_rate=6.34e-04,
-        n_steps=2048,
+        learning_rate=2.5e-4,
+        n_steps=4096,
         batch_size=512,
         n_epochs=23,
-        gamma=0.98,
-        clip_range=0.22,
-        ent_coef=4.50e-04,
+        gamma=0.99,
+        clip_range=0.1,
+        ent_coef=0.01,
+        policy_kwargs=dict(net_arch=[dict(pi=[128, 128], vf=[128, 128])]),
         verbose=1,
         tensorboard_log=log_dir
     )
diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
index 2486f225..c902c9cd 120000
--- a/wandb/debug-internal.log
+++ b/wandb/debug-internal.log
@@ -1 +1 @@
-run-20250515_153558-cbk4o9hz/logs/debug-internal.log
\ No newline at end of file
+run-20250517_205529-1kw0mq15/logs/debug-internal.log
\ No newline at end of file
diff --git a/wandb/debug.log b/wandb/debug.log
index c6b5a8f5..e3408f9f 120000
--- a/wandb/debug.log
+++ b/wandb/debug.log
@@ -1 +1 @@
-run-20250515_153558-cbk4o9hz/logs/debug.log
\ No newline at end of file
+run-20250517_205529-1kw0mq15/logs/debug.log
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
index 86ff72a4..b6a35bf2 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20250515_153558-cbk4o9hz
\ No newline at end of file
+run-20250517_205529-1kw0mq15
\ No newline at end of file
