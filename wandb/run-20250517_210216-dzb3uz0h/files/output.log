Using cpu device
/Users/jintpongchababnapa/Documents/Dissertation/agent/venv310/lib/python3.10/site-packages/stable_baselines3/common/policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])
  warnings.warn(
Training ppo_model_500000 for 500000 timesteps...
Logging to ./train_output/tensorboard_logs_500000/PPO_8
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 196       |
|    ep_rew_mean     | -2.45e+05 |
| time/              |           |
|    fps             | 472       |
|    iterations      | 1         |
|    time_elapsed    | 8         |
|    total_timesteps | 4096      |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 196         |
|    ep_rew_mean          | -2.33e+05   |
| time/                   |             |
|    fps                  | 458         |
|    iterations           | 2           |
|    time_elapsed         | 17          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.002034579 |
|    clip_fraction        | 0.0615      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.691      |
|    explained_variance   | -0.38       |
|    learning_rate        | 0.00025     |
|    loss                 | 0.127       |
|    n_updates            | 23          |
|    policy_gradient_loss | -0.00516    |
|    value_loss           | 0.545       |
-----------------------------------------
Traceback (most recent call last):
  File "/Users/jintpongchababnapa/Documents/Dissertation/agent/train.py", line 140, in <module>
    ppo_model.learn(total_timesteps=train_timesteps, callback=ppo_reward_logging)
  File "/Users/jintpongchababnapa/Documents/Dissertation/agent/ppo2.py", line 231, in learn
    return super().learn(
  File "/Users/jintpongchababnapa/Documents/Dissertation/agent/venv310/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 324, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "/Users/jintpongchababnapa/Documents/Dissertation/agent/venv310/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 218, in collect_rollouts
    new_obs, rewards, dones, infos = env.step(clipped_actions)
  File "/Users/jintpongchababnapa/Documents/Dissertation/agent/venv310/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py", line 222, in step
    return self.step_wait()
  File "/Users/jintpongchababnapa/Documents/Dissertation/agent/venv310/lib/python3.10/site-packages/stable_baselines3/common/vec_env/vec_normalize.py", line 181, in step_wait
    obs, rewards, dones, infos = self.venv.step_wait()
  File "/Users/jintpongchababnapa/Documents/Dissertation/agent/venv310/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 59, in step_wait
    obs, self.buf_rews[env_idx], terminated, truncated, self.buf_infos[env_idx] = self.envs[env_idx].step(  # type: ignore[assignment]
  File "/Users/jintpongchababnapa/Documents/Dissertation/agent/venv310/lib/python3.10/site-packages/stable_baselines3/common/monitor.py", line 94, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
  File "/Users/jintpongchababnapa/Documents/Dissertation/agent/aquacropgymnasium/env.py", line 105, in step
    self.model.run_model(initialize_model=False)
  File "/Users/jintpongchababnapa/Documents/Dissertation/agent/venv310/lib/python3.10/site-packages/aquacrop/core.py", line 298, in run_model
    ) = self._perform_timestep()
  File "/Users/jintpongchababnapa/Documents/Dissertation/agent/venv310/lib/python3.10/site-packages/aquacrop/core.py", line 325, in _perform_timestep
    new_cond, param_struct, outputs = solution_single_time_step(
  File "/Users/jintpongchababnapa/Documents/Dissertation/agent/venv310/lib/python3.10/site-packages/aquacrop/timestep/run_single_timestep.py", line 435, in solution_single_time_step
    Wr, _water_root_depletion.Zt, _water_root_depletion.Rz, _TAW.Zt, _TAW.Rz, _, _, _, _, _, _ = root_zone_water(
  File "/Users/jintpongchababnapa/Documents/Dissertation/agent/venv310/lib/python3.10/site-packages/aquacrop/solution/root_zone_water.py", line 98, in root_zone_water
    WrAer = WrAer + round(factor * 1000 * (prof.th_s[ii] - (Crop_Aer / 100)) * prof.dz[ii], 2)
KeyboardInterrupt
